{
    "contents" : "KMeans\n========================================================\n\n### Clustering cluster.csv\n\nUse kmeans() w/ default values to find the k=2 solution for the 2-dimensional data cluster.csv\n\n\n```{r}\nsetwd(\"./data\")\ndata<-read.csv(\"cluster.csv\",header=F)\nsetwd(\"../\")\n```\n\nPlot initial data.\n\n```{r fig.width=7, fig.height=6}\nplot(data)\n```\n\nCluster: kmeans() produces centers, cluster assignments, etc.\n\n```{r}\nfit<-kmeans(data,2)\nfit\nfit$centers\n```\n\nPlot: Key here is to use kmeans() output rather than moving data around. \n\n```{r fig.width=7, fig.height=6}\nplot(data$V1,data$V2,col=fit$cluster)\npoints(fit$centers,col=c(\"black\",\"red\"),pch=19)\n```\n\n### Clustering sonar\n\nUse kmeans() w/ default values to find the k=2 solution for the 2-dimensional sonar data.\n\n\n```{r}\nsetwd(\"./data\")\ntrain<-read.csv(\"sonar_train.csv\",header=F)\ntest<-read.csv(\"sonar_test.csv\",header=F)\nsetwd(\"../\")\n```\n\nPlot just the first two columns of the sonar data.\n\n```{r}\nplot(train[,1:2])\n```\n\nCluster: kmeans() can use as many attributes as you want. But, let's look at the clusters created by the first two. \n\n```{r}\nfit<-kmeans(train[,1:2],2)\nfit\n```\n\nPlot: Key here is to use kmeans() output rather than moving data around. \n\n```{r fig.width=7, fig.height=6}\nplot(train[,1:2],col=fit$cluster)\npoints(fit$centers,col=\"blue\",pch=19)\n```\n\n### Compare sonar clusters to actual class labels??\n\n\n```{r fig.width=7, fig.height=6}\nplot(train[,1:2],pch=19,xlab=expression(x[1]),\n     ylab=expression(x[2]))\n## get your y labels\ny<-train[,61]\n## re-plot points with color based on class labels.\npoints(train[,1:2],col=2+2*y,pch=19)\n```\n\n### Compute the misclass error\n\nWhat if we used kmeans() to classify. What would our misclass error be?\n\n\n```{r}\n## transform cluster labels (1's and 2's) to -1s and 1s\nsum(fit$cluster*2-3==y)/length(y)\n\n```\n\n### Try it for all 60 columns\n\n```{r}\nfit<-kmeans(train[,1:60],2)\nsum(fit$cluster*2-3==y)/length(y)\nsum(fit$cluster*2-3!=y)/length(y)\n```\n\nTry w/ more centroids. Disaster.\n\n```{r}\nfit<-kmeans(train[,1:60],10)\nsum(fit$cluster*2-3==y)/length(y)\nsum(fit$cluster*2-3!=y)/length(y)\n```\n\nGist: kmeans() is a good clustering tool. Not a good prediction tool.\n\n### What is kmeans doing? \n\nFirst code it manually. \n\n```{r}\nx<-c(1,2,3,5,6,7,8)\ncenter1<-1\ncenter2<-2\n\nfor (k in 2:10) {\n     cluster1<-x[abs(x-center1[k-1])<=abs(x-center2[k-1])]\n     ## Put in cluster1 all x's where distance to c1<= distance to c2.\n     cluster2<-x[abs(x-center1[k-1])>abs(x-center2[k-1])]\n     ## Put in c2 all x's where distance to c1>distance to c2\n     \n     center1[k]<-mean(cluster1)\n     center2[k]<-mean(cluster2)\n     ## apparently mean() will take the mean between of all values in a cluster.\n     ## set k=2. Decrement it 1 to control iteration. Also use it to track the updates clusters. \n}\n\ncenter1\ncenter2\ncluster1\ncluster2\n```\n\nCompare to kmeans()\n\n```{r}\nx<-c(1,2,3,5,6,7,8)\n\nfit<-kmeans(x,2)\n\nplot(x,col=fit$cluster)\n```\n\n### Calc distances\n\n```{r}\nx1<-c(2,2)\nx2<-c(5,7)\n\ndata<-matrix(c(x1,x2),nrow=2,byrow=T)\ndata\n\ndist(data)\n```\n\n",
    "created" : 1407003504067.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3495671275",
    "id" : "49AF8D61",
    "lastKnownWriteTime" : 1407040827,
    "path" : "~/stats202lectureR/kMeans.Rmd",
    "project_path" : "kMeans.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}