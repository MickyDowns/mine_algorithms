{
    "contents" : "hw4p6b\n========================================================\n\n### Load test and train data\n```{r load}\nsetwd(\"./data\")\ntrain<-read.csv(\"sonar_train.csv\",header=FALSE)\ntest<-read.csv(\"sonar_test.csv\",header=FALSE)\nsetwd(\"../\")\n\ny<-train[,61]\nx<-train[,1:60]\ny_test<-test[,61]\nx_test<-test[,1:60]\n```\n\n## Using baseline alpha\n\n### Initialize counters\n \n```{r initialize}\niter=500\n\ntrain_error<-rep(0,iter) # Keep track of errors\ntest_error<-rep(0,iter)\nf<-rep(0,130) # 130 pts in training data\nf_test<-rep(0,78) # 78 pts in test data\n\nsumW<-rep(0,iter)\nsumW_test<-rep(0,iter)\n\nsumWnorm<-rep(0,iter)\nsumWnorm_test<-rep(0,iter)\n```\n\n### Process Baseline Adboost loop\n\n```{r baseLoop}\ni<-1\nlibrary(rpart)\n\nwhile(i<=iter){\n     w<-exp(-y*f) # This is a shortcut to compute w\n     sumW[i]<-sum(w)\n     \n     w_test<-exp(-y_test*f_test)\n     sumW_test[i]<-sum(w_test)\n     \n     w<-w/sum(w)\n     sumWnorm[i]<-sum(w)\n     \n     w_test<-w_test/sum(w_test)\n     sumWnorm_test[i]<-sum(w_test)\n\n     fit<-rpart(y~.,x,w,method=\"class\")\n\n     g<--1+2*(predict(fit,x)[,2]>.5) # make -1 or 1\n     g_test<--1+2*(predict(fit,x_test)[,2]>.5)\n\n     e<-sum(w*(y*g<0))\n     \n     alpha<-.5*log ( (1-e) / e )\n\n     f<-f+alpha*g\n     f_test<-f_test+alpha*g_test\n\n     train_error[i]<-sum(1*f*y<0)/130\n     test_error[i]<-sum(1*f_test*y_test<0)/78\n\n     i<-i+1\n}\nprint(min(test_error)) # minimum test error\nprint(which.min(test_error)) # minimum test error iteration\nprint(test_error[iter]) # final test error\n\nsumW_base<-sumW\nsumW_test_base<-sumW_test\ntrain_error_base<-train_error\ntest_error_base<-test_error\n```\n\n## Using shrink alpha\n\n### Re-initialize counters\n \n```{r reinitialize}\niter=500\n\ntrain_error<-rep(0,iter) # Keep track of errors\ntest_error<-rep(0,iter)\nf<-rep(0,130) # 130 pts in training data\nf_test<-rep(0,78) # 78 pts in test data\n\nsumW<-rep(0,iter)\nsumW_test<-rep(0,iter)\n\nsumWnorm<-rep(0,iter)\nsumWnorm_test<-rep(0,iter)\n```\n\n### Process shrink Adboost loop\n\n```{r shrinkLoop}\ni<-1\n\nwhile(i<=iter){\n     w<-exp(-y*f) # This is a shortcut to compute w\n     sumW[i]<-sum(w)\n     \n     w_test<-exp(-y_test*f_test)\n     sumW_test[i]<-sum(w_test)\n     \n     w<-w/sum(w)\n     sumWnorm[i]<-sum(w)\n     \n     w_test<-w_test/sum(w_test)\n     sumWnorm_test[i]<-sum(w_test)\n\n     fit<-rpart(y~.,x,w,method=\"class\")\n\n     g<--1+2*(predict(fit,x)[,2]>.5) # make -1 or 1\n     g_test<--1+2*(predict(fit,x_test)[,2]>.5)\n\n     e<-sum(w*(y*g<0))\n     \n     alpha<-0.1*(.5*log ( (1-e) / e ))\n\n     f<-f+alpha*g\n     f_test<-f_test+alpha*g_test\n\n     train_error[i]<-sum(1*f*y<0)/130\n     test_error[i]<-sum(1*f_test*y_test<0)/78\n\n     i<-i+1\n}\nprint(min(test_error)) # minimum test error\nprint(which.min(test_error)) # minimum test error iteration\nprint(test_error[iter]) # final test error\n\nsumW_shrink<-sumW\nsumW_test_shrink<-sumW_test\ntrain_error_shrink<-train_error\ntest_error_shrink<-test_error\n```\n\n## Output errors graphic\n\n```{r plotBaseErrors}\n# plot log(w)\nplot(seq(1,iter),test_error_shrink,type=\"l\",ylim=c(0,0.4),ylab=\"Error Rate\",\n     xlab=\"Iterations\",lwd=2,col=\"red4\",main=\"Sonar Train & Test Errors with/without Shrinkage (MichaelDowns\")\n\nlines(test_error_base,type=\"l\",col=\"red1\",lwd=3)\nlines(train_error_shrink,type=\"l\",col=\"blue\",lwd=3)\nlines(train_error_base,type=\"l\",col=\"lightblue\",lwd=3)\n\nlegend(\"topright\",0.5,\n       c(\"Test Error w/ Shrinkage\", \"Test Error Baseline (no shrink)\",\"Train Error w/ Shrinkage\",\"Train Error Baseline (no shrink\"),\n       col=(c(\"red4\",\"red1\",\"blue\",\"lightblue\")),lwd=3)\n```\n\n### Output log(sum(w)) graphic\n\n```{r plotLogw}\n# plot log(w)\nplot(seq(1,iter),log(sumW_test_shrink),type=\"l\",\n     ylab=\"Sum of Dataset Exponential Loss (log(sum(w)))\",\n     xlab=\"Iterations\",col=\"red4\",lwd=2,ylim=c(0,250),\n     main=\"Sonar Train & Test Exponential Loss with/without Shrinkage (Michael Downs)\")\n\nlines(log(sumW_test_base),type=\"l\",col=\"red1\",lwd=3)\nlines(log(sumW_shrink),type=\"l\",col=\"blue\",lwd=3)\nlines(log(sumW_base),type=\"l\",col=\"lightblue\",lwd=3)\n\nlegend(\"topleft\",0.5,\n       c(\"Test Data w/ Shrinkage\", \"Test Data Baseline (no shrink)\",\"Train Data w/ Shrinkage\",\"Train Data Baseline (no shrink\"),\n       col=(c(\"red4\",\"red1\",\"blue\",\"lightblue\")),lwd=3)\n```\n",
    "created" : 1407631353864.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3680350982",
    "id" : "1BAFFB18",
    "lastKnownWriteTime" : 1407647379,
    "path" : "~/stats202lectureR/hw4p6bShrink.Rmd",
    "project_path" : "hw4p6bShrink.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}